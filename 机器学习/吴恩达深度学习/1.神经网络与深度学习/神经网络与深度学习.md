<!-- TOC -->

- [1.神经网络基础](#1神经网络基础)
  - [1.1 二分分类](#11-二分分类)
  - [1.2 logistic回归 （逻辑回归）](#12-logistic回归-逻辑回归)
  - [1.3 logistic回归损失函数 （逻辑回归损失函数）](#13-logistic回归损失函数-逻辑回归损失函数)
    - [成本函数 Cost function](#成本函数-cost-function)

<!-- /TOC -->
# 1.神经网络基础
## 1.1 二分分类
假如我们有m个样本的训练集，我们可能会习惯使用一个for循环来遍历这m个样本，但事实上实现一个神经网络并不需要我们直接使用for循环 
神经网络的计算过程通常有一个正向过程 （也叫正向传播步骤），接着会有一个反向步骤 （也叫反向传播步骤） 
什么叫二分类？
> 简单来说就是一个问题最终只有两种结果，比如我们识别一只猫，那么识别的结果就只有两种，yes or no

我们用一对(x,y)来表示一个单独的样本 ，其中x是nx维的特征向量，标签y值为0或1，而我们的训练集由m个训练样本构成，其中
$$(x^1，y^1)$$表示样本1的输入和输出，同理$$(x^2，y^2)$$表示样本2的输入和输出,....以此类推$$(x^m，y^1)$$表示样本m的输入和输出，这些一起就构成了整个训练集
我们用整个m来表示训练样本的个数，为了区分训练样本和测试样本，我们使用：
* m_train:训练样本个数
* m_test:测试集样本个数

我们接着定义一个X矩阵，它是由训练集中的x1,x2,x3....组成
$$
X=\left[
\begin{matrix}
. & . & .... & . \\
. & . & ... & . \\
x^1 & x^2 & ... & x^m \\
. & . & .... & . \\
. & . & ... & . 
\end{matrix} \right]\tag{2}
$$
这个矩阵有m列，m就是我们的训练样本数，高记为nx，但是需要我们注意的是有时候矩阵的定义是训练样本作为行向量堆叠，而不是这样列向量堆叠，于是我们将上述矩阵稍作变换，即我们使用向量的转置变成行向量来构建神经网络
输出X.shape，会得到（nx * m）的结果
![](1.JPG)
将输入矩阵说明之后，那么我们的输出矩阵Y应该怎么表示呢？
我们同样将y标签也放在列中，定义Y是由$$y^(1),y^(2),...,y^(m)$$组成的矩阵，$$Y=\left[
\begin{matrix}
y^(1) & y^(2) & .... & y^(m) 
\end{matrix} \right]\tag{3}$$
如果我们输出Y.shape 会得到（1，m）

## 1.2 logistic回归 （逻辑回归）
逻辑回归就是来解决二元分类乃至多元分类问题的一个算法
我们给出X,希望得到一个预测值y，我们将这个预测值定义为$$\overline{y}$$,实际上这个预测值是一个概率，我们希望$$0<=\overline{y}<=1$$,如果我们继续采用线性回归$$\overline{y}=w^Tx+b，其中w也是nx维度，b是任意一个实数$$的方式来解决多元分类的问题就会得到负值或者大于1的值，但是这些值对我们来说是没有任何意义的，所以我们在这个函数的基础上添加一个激活函数变为$$\overline{y}=\sigma(w^Tx+b)$$,我们看一下激活函数的曲线：
![](2.jpg)
而$$\sigma(z)=\frac{1}{1+e^-z}$$当z非常大时，这个函数趋于1，当z=0时，函数值为0.5,当z非常小时，函数值无限趋于0
当我们对神经网络编程时，我们通常会把w和参数b分开，其中b对应一个拦截器
## 1.3 logistic回归损失函数 （逻辑回归损失函数）
损失函数的作用：他们用来衡量算法的运行情况
如果我们继续使用梯度下降的损失函数![](3.png)实际上在处理多元模型时只能得到局部最优解，但是我们定义损失函数的目的主要是为了衡量预测输出值y^和实际值y有多接近
我们先看一下损失函数的形态：
$$L(\overline{y},y)=-(y\log^{\overline{y}}+(1-y)\log^{1-\overline{y}})$$
我们要实现的就是让损失函数尽可能的小
我们先看两个例子：
* y=1 1-y=0,所以$$L(\overline{y},y)=-(y\log^{\overline{y}}+0)$$我们要想L尽可能的小，则$$\log^{\overline{y}}$$尽可能的大，意味着y^  尽可能的大，但是y^ 是我们由激活函数$$\sigma(z)=\frac{1}{1+e^-z}$$求出，最大也不会超过1，所以我们要尽可能的让y^ 接近1
* y=0 ,则$$L(\overline{y},y)=-(0+\log^{1-\overline{y}})$$要是L尽可能的小，则需要y^ 尽可能的小，但是0< y^ <1 所以我们需要让y^ 尽可能的接近0
### 成本函数 Cost function
接下来我们引入成本函数，它的作用是在全体训练样本上的表现，而我们的损失函数是在单个训练样本上的表现
下面我们具体了解一下这个成本函数：
